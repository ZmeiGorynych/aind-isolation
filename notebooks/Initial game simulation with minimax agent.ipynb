{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tournament import tournament, Agent, CustomPlayerComp, improved_score_fast_x2\n",
    "from data_utils import get_depths\n",
    "discount_factor = 0.99\n",
    "\n",
    "def run_tournament(trainee_, num_rounds = 10, time_limit=float('inf') ):\n",
    "    result = tournament(num_rounds, test_agents = [trainee_], time_limit = time_limit)\n",
    "    #print(result)\n",
    "    nice_data = get_depths(result, [trainee_], lambda x:x, discount_factor)\n",
    "    #print(nice_data)\n",
    "    # TODO: nicer handling of final states, so wins/losses also propagate from final values via Q, not just via G\n",
    "    states = [state for game in nice_data[trainee_.name] for state in game]\n",
    "    print('imported',len(states), 'states')\n",
    "    return states, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_size = 100000\n",
    "batch_size = 10000\n",
    "train_batch_size = 128\n",
    "num_rounds = 100\n",
    "num_init_rounds = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neural.reinforcement import Memory\n",
    "memory = Memory(mem_size)\n",
    "dummy_loss = 1.0 # later want to oversample high losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*************************\n",
      "   Evaluating: Trainee   \n",
      "*************************\n",
      "\n",
      "Playing Matches:\n",
      "----------\n",
      "  Match 1:   Trainee   vs   Random    \tResult: 312 to 88\n",
      "  Match 2:   Trainee   vs   MM_Null   \tResult: 253 to 147\n",
      "  Match 3:   Trainee   vs   MM_Open   \tResult: 208 to 192\n",
      "  Match 4:   Trainee   vs MM_Improved \tResult: 216 to 184\n",
      "  Match 5:   Trainee   vs   AB_Null   \tResult: 254 to 146\n",
      "  Match 6:   Trainee   vs   AB_Open   \tResult: 246 to 154\n",
      "  Match 7:   Trainee   vs AB_Improved \tResult: 228 to 172\n",
      "\n",
      "\n",
      "Results:\n",
      "----------\n",
      "Trainee             61.32%\n",
      "[0.966032673953495, 0.97549502495, 0.9851495, 0.9476691271293582, 0.995, 0.9567586237418204, 0.047808962495597795, 0.03862765278603997, 0.0292599252995, 0.019701995000000028, 0.009950000000000014, 0.0, 0.9387605114994839, 0.06562709361551089, 0.056807564141935385]\n",
      "imported 13842 states\n"
     ]
    }
   ],
   "source": [
    "# Run this to generate the initial simulation data to pre-fit the model\n",
    "trainee = Agent(CustomPlayerComp(score_fn=improved_score_fast_x2, \n",
    "                                 name = \"Trainee\", \n",
    "                                 method ='alphabeta', \n",
    "                                 iterative = True), \n",
    "                \"Trainee\")\n",
    "\n",
    "states, result = run_tournament(trainee,num_init_rounds, time_limit = 150)\n",
    "\n",
    "for state in states:\n",
    "    memory.add((state, dummy_loss))\n",
    "\n",
    "import pickle\n",
    "with open('../data/initial_run.pickle', 'wb') as handle:\n",
    "    pickle.dump(states, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from neural.keras_utils import deep_model_fun\n",
    "from neural.neural_agent import NeuralAgent\n",
    "def model_fun():\n",
    "    return deep_model_fun(num_features =32, num_res_modules = 8, drop_rate = 0.1, activation = 'sigmoid')\n",
    "\n",
    "def create_neural_agent(model_fun, name = None):\n",
    "    deep_model, deep_Q_model = model_fun() \n",
    "    #deep_Q_model.load_weights(filename)#,custom_objects={'ConvByMoveLayer': ConvByMoveLayer})\n",
    "    deep_model.compile(optimizer = 'adam',  loss='mean_squared_error')\n",
    "    my_agent = NeuralAgent(deep_Q_model, name = name)\n",
    "    return Agent(my_agent,name), deep_model, deep_Q_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tournament import tournament, Agent, RandomPlayer\n",
    "my_agent, deep_model, deep_Q_model = create_neural_agent(model_fun, name = 'Trainee')\n",
    "\n",
    "filename = '../data/deep_Q_model_weights.h5'\n",
    "deep_Q_model.load_weights(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41526\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 9s - loss: 0.0662 - val_loss: 0.0515\n",
      "\n",
      "*************************\n",
      "   Evaluating: Trainee   \n",
      "*************************\n",
      "\n",
      "Playing Matches:\n",
      "----------\n",
      "  Match 1:   Trainee   vs   Random    \tResult: 278 to 122\n",
      "  Match 2:   Trainee   vs   MM_Null   \tResult: 195 to 205\n",
      "  Match 3:   Trainee   vs   MM_Open   "
     ]
    }
   ],
   "source": [
    "from neural.reinforcement import generate_target\n",
    "\n",
    "for state in states:\n",
    "    memory.add((state, dummy_loss))\n",
    "print(len(memory.buffer))\n",
    "\n",
    "\n",
    "idx,batch_states = memory.sample(batch_size)\n",
    "batch_states =[b[0] for b in batch_states]\n",
    "\n",
    "board_full, player_pos, legal_moves, next_move, target = generate_target(batch_states, deep_Q_model, alpha=1.0, discount_factor=0.99)\n",
    "deep_model.fit([board_full, player_pos, legal_moves, next_move], \n",
    "               target, \n",
    "               batch_size = train_batch_size, \n",
    "               epochs=1, \n",
    "               verbose =1, \n",
    "               validation_split = 0.2, \n",
    "               shuffle = True)\n",
    "\n",
    "states, result = run_tournament(my_agent,num_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aind]",
   "language": "python",
   "name": "conda-env-aind-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
